{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Урок 2. Парсинг HTML. BeautifulSoup, MongoDB\n",
    "\n",
    "1) Необходимо собрать информацию о вакансиях на вводимую должность (используем input или через аргументы) с сайта `superjob.ru` и `hh.ru`. Приложение должно анализировать несколько страниц сайта(также вводим через input или аргументы). Получившийся список должен содержать в себе минимум:\n",
    "\n",
    "    *Наименование вакансии\n",
    "    *Предлагаемую зарплату (отдельно мин. и отдельно макс. и отдельно валюта)\n",
    "    *Ссылку на саму вакансию        \n",
    "    *Сайт откуда собрана вакансия\n",
    "По своему желанию можно добавить еще работодателя и расположение. Данная структура должна быть одинаковая для вакансий с обоих сайтов. Общий результат можно вывести с помощью `dataFrame` через `pandas`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Решение\n",
    "В домашней работе выполнял поиск вакансии для ***C++ Developer***. \n",
    "Каждый из сайтов в названии вакансии заменяет часть символов из строки поискового запроса специфическими символоами форматирования. Поэтому, чтобы не увеличиваить объем кода, я не стал делать преобразования, а просто подставил эти строки как есть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Safari/537.36',\n",
    "    'charset': 'utf-8'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacancy = [] # Словарь в котором будет храниться список вакансий.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selary_analizer(selary_txt):  # возвращает массив содержащий значение зарбплаты.\n",
    "    min_selary = None\n",
    "    max_selary = None\n",
    "    money_type = None\n",
    "    symbols = ['от', 'до']\n",
    "    # print(selary_txt)\n",
    "    if selary_txt != '' and selary_txt != 'По договорённости':  # 'проверяем наличие ключевых символов в строке'\n",
    "        selary_txt = selary_txt.replace('—\\xa0', '')\n",
    "        selary_txt = selary_txt.replace('-', ' ')\n",
    "        selary_txt = selary_txt.replace('\\xa0', ' ')\n",
    "        selary_txt = selary_txt.replace('/месяц', '')\n",
    "        selary_array = selary_txt.split(' ')\n",
    "        money_type = selary_array[-1].rstrip('.')\n",
    "\n",
    "        if selary_array[0] == symbols[0]: # от\n",
    "            min_selary = selary_array[1] + selary_array[2]\n",
    "        elif (selary_array[0] == symbols[1]):  # до\n",
    "            max_selary = selary_array[1] + selary_array[2]\n",
    "        elif (len(symbols) < 4):  # до\n",
    "            max_selary = selary_array[0] + selary_array[1]\n",
    "        else:                                           \n",
    "            min_selary = selary_array[0] + selary_array[1]\n",
    "            max_selary = selary_array[2] + selary_array[3]\n",
    "\n",
    "    return [min_selary, max_selary, money_type]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поиск Вакансии на сайте  https://hh.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_link = 'https://izhevsk.hh.ru/search/vacancy?L_is_autosearch=false&area=113&clusters=true&enable_snippets=true'\n",
    "page = 0\n",
    "is_next_page = True  # Флаг поназываюций что есть еще страницы.\n",
    "# Выполняем переход на следующую страницу.\n",
    "# Может существовать как минимум одна страница с запросом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'C%2B%2B+Developer'  # 'Machine+Learning+Engineer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найдено вакансий: 330 "
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "while is_next_page == True:\n",
    "    response = requests.get(f'{main_link}&text={text}&page={page}', headers=header).text\n",
    "    soup = bs(response, 'lxml')\n",
    "\n",
    "    btn_next = soup.find('a', {'class': \"HH-Pager-Controls-Next\"})\n",
    "\n",
    "    if btn_next != None:\n",
    "#         print(f'Cтраница {page + 1}',end=\" \")\n",
    "        page += 1\n",
    "    else:\n",
    "#         print(f'Cтраница {page + 1}',end=\" \")\n",
    "        is_next_page = False\n",
    "\n",
    "    vacancy_block = soup.find('div', {'class': 'vacancy-serp'})\n",
    "    divs = vacancy_block.findChildren(recursive=False)\n",
    "\n",
    "\n",
    "    for vk in divs:\n",
    "        vacancy_data = {}\n",
    "        i += 1\n",
    "        try:\n",
    "            if vk.attrs['class'][0] != 'serp-special':  # Проверка на то не являетс яли даггый тег рекламой.\n",
    "                vacancy_name = vk.find('div', {'class': 'vacancy-serp-item__info'}).getText()\n",
    "                # Ссылка на вакансию.\n",
    "                vk_href = vk.find('a', {'class': 'bloko-link HH-LinkModifier'})['href']\n",
    "                # Нужно создать функцию для разбиения строки. на составляющие.\n",
    "                selary_txt = vk.find('div', {'class': 'vacancy-serp-item__sidebar'}).getText()\n",
    "                # Название фирмы.\n",
    "                vk_employer = vk.find('a', {'data-qa': \"vacancy-serp__vacancy-employer\"}).getText()\n",
    "                vk_employer_id = 'https://izhevsk.hh.ru' + vk.find('a', {'data-qa': \"vacancy-serp__vacancy-employer\"})[\n",
    "                    'href']\n",
    "                # Город расположения.\n",
    "                vk_location = vk.find('span', {'data-qa': \"vacancy-serp__vacancy-address\"}).getText()\n",
    "                # Дата публикации.\n",
    "                vacancy_date = vk.find('span', {'data-qa': \"vacancy-serp__vacancy-date\"}).getText()\n",
    "\n",
    "                min_selary, max_selary, money_type = selary_analizer(selary_txt)\n",
    "\n",
    "                vacancy_data['name'] = vacancy_name\n",
    "                vacancy_data['employer'] = vk_employer\n",
    "                vacancy_data['location'] = vk_location\n",
    "                vacancy_data['min_selary'] = min_selary\n",
    "                vacancy_data['max_selary'] = max_selary\n",
    "                vacancy_data['money_type'] = money_type\n",
    "                # vacancy_data['publicatin_date'] = vacancy_date\n",
    "                vacancy_data['href'] = vk_href\n",
    "                vacancy_data['employer_id'] = vk_employer_id\n",
    "                vacancy_data['source'] = 'https://hh.ru/'\n",
    "                vacancy.append(vacancy_data)\n",
    "        except Exception as ex:\n",
    "            print(\"\")\n",
    "            print('Ошибка при анализе вакансии ', ex)\n",
    "        print(\"\", end=\"\\r\")\n",
    "        print(f'Найдено вакансий: {i}',end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поиск вакансии на сайте https://russia.superjob.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_link = 'https://russia.superjob.ru/vacancy/search/'\n",
    "page = 1 # Отсчет начинается с 1. \n",
    "is_next_page = True  # Флаг поназываюций что есть еще страницы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'C%2B%2B%20Developer' #'Machine%20Learning%20Engineer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "while (is_next_page == True):\n",
    "    response = requests.get(f'{main_link}?keywords={text}&page={page}', headers=header).text\n",
    "\n",
    "    soup = bs(response, 'lxml')\n",
    "\n",
    "    btn_next = soup.findAll('span', {'class': \"qTHqo _1mEoj _2h9me DYJ1Y _2FQ5q _2GT-y\"})\n",
    "    if 'Дальше' == btn_next[-1].getText():\n",
    "#         print(f'Cтраница {page}')\n",
    "        page += 1\n",
    "    else:\n",
    "#         print(f'Cтраница {page}')\n",
    "        is_next_page = False\n",
    "\n",
    "    vacancy_hrefs = soup.find('script',\n",
    "                              {'type': \"application/ld+json\"}).string  # Преобразуем полученное значение в  json.\n",
    "\n",
    "    v_links = json.loads(vacancy_hrefs)\n",
    "    v_link_list = []\n",
    "    for v_link in v_links['itemListElement']:\n",
    "        v_link_list.append(v_link['url'])\n",
    "\n",
    "\n",
    "    # Последовательно загружаем сайты с описанием вакансии и считываем с него информацию.\n",
    "\n",
    "    for v_link in v_link_list:\n",
    "        i += 1\n",
    "        vacancy_data = {}\n",
    "        try:\n",
    "            v_response = requests.get(v_link, headers=header).text\n",
    "\n",
    "            v_soup = bs(v_response, 'lxml')\n",
    "            vs = v_soup.find('div', {'class': '_3Qutk'})\n",
    "            vsk = vs.find_next('div', {'class': '_3MVeX'})\n",
    "\n",
    "            vacancy_name = vsk.contents[1].getText()  \n",
    "            # Ссылка на вакансию.\n",
    "            vk_href = v_link\n",
    "            selary_txt = vsk.contents[4].getText()\n",
    "            # Город расположения.\n",
    "            vk_location = vsk.contents[2].getText()\n",
    "\n",
    "            vs2 = vs.find_next('div', {'class': '_3zucV undefined'})\n",
    "            # Название фирмы.\n",
    "            vk_employer_id = None\n",
    "            vk_employer = vs2.find('a', {'class': 'icMQ_'})\n",
    "            if vk_employer is not None:\n",
    "                vk_employer_id = 'https://russia.superjob.ru' + vk_employer['href']\n",
    "                vk_employer = vk_employer.getText()\n",
    "            else:\n",
    "                vk_employer = vs.find('span', {'class': '_3mfro _1hP6a _2JVkc _2VHxz'}).getText()\n",
    "\n",
    "            min_selary, max_selary, money_type = selary_analizer(selary_txt)\n",
    "            vacancy_data['name'] = vacancy_name\n",
    "            vacancy_data['employer'] = vk_employer\n",
    "            vacancy_data['location'] = vk_location\n",
    "            vacancy_data['min_selary'] = min_selary\n",
    "            vacancy_data['max_selary'] = max_selary\n",
    "            vacancy_data['money_type'] = money_type\n",
    "            # vacancy_data['publicatin_date'] = vacancy_date # не смог найти где дата публикации указана.\n",
    "            vacancy_data['href'] = vk_href\n",
    "            vacancy_data['employer_id'] = vk_employer_id\n",
    "            vacancy_data['source'] = 'https://russia.superjob.ru'\n",
    "            vacancy.append(vacancy_data)\n",
    "        except Exception as ex:\n",
    "            print(\"\")\n",
    "            print('Ошибка при анализе вакансии ', ex)\n",
    "        print(\"\", end=\"\\r\")\n",
    "        print(f'Найдено вакансий: {i}',end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(vacancy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Сохраняем результаты на в файл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Vacanсies.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
